# ğŸ§  RAGAS Evaluation vs BLEU, ROUGE, BERTScore & MMI/TOIR

This project demonstrates how to evaluate **RAG (Retrieval-Augmented Generation)** pipelines using **RAGAS** â€” a modern, LLM-agnostic metric designed to analyze **answer relevancy**, **faithfulness**, and **hallucination detection** â€” compared to traditional NLP metrics like **BLEU**, **ROUGE**, **BERTScore**, and **MMI/TOIR**.

---

## ğŸ¯ Why RAGAS?

Traditional metrics were not designed to evaluate answers from retrieval-based systems. They fail in these key areas:

| Metric     | Context Aware | Faithfulness | Hallucination Detection | Explanation |
|------------|----------------|---------------|---------------------------|-------------|
| BLEU       | âŒ              | âŒ             | âŒ                         | Measures token-level overlap |
| ROUGE      | âŒ              | âŒ             | âŒ                         | Based on recall; not contextual |
| BERTScore  | âœ…              | âŒ             | âŒ                         | Uses embeddings but lacks grounding |
| MMI/TOIR   | âœ…              | âš ï¸              | âš ï¸                         | Hard to scale; not explainable |
| **RAGAS**  | âœ…              | âœ…             | âœ…                         | Multi-dimensional, LLM-optional, explainable |

---

## ğŸš€ Features

- âœ… Full RAGAS pipeline implementation in **Python + VSCode**
- ğŸ“‚ CSV evaluation on `question`, `ground_truth`, `context`, `predicted_answer`
- âš™ï¸ Modular code with `pydantic`, `langchain`, and `transformers`
- ğŸ“Š Visualization via **Streamlit**
- ğŸ”„ Compare RAGAS scores with BLEU, ROUGE, BERTScore, and MMI/TOIR

---

## ğŸ§ª Requirements

Install all dependencies using:

```bash
pip install -r requirements.txt
